![ASL Project Banner](asl_img.png)
# ğŸ§  American Sign Language (ASL) Recognition from Hand Gestures

A real-time, production-inspired deep learning application for recognizing American Sign Language (ASL) from hand gestures using optimized video processing, object detection, and image classification. Developed in a **version-based modular pipeline**, this project demonstrates how to build, optimize, and deploy a scalable gesture recognition system that can be adapted for real-world use cases like accessibility tools, education, and human-computer interaction.

---

## ğŸš€ Version-Controlled Pipeline Development

### âœ… Version 1: Foundational Skeleton
Established a modular pipeline that initializes the object detection and image classification components, sets up video sources, and prepares the structure for further integration. This foundational structure reflects how real-time systems are architected before integrating functionality.

### âœ… Version 2: Integration of Object Detection into Live Video
Connected object detection directly into the live video stream pipeline. This enabled the system to detect hands in real-time, introducing minor latency. The integration mimics how production pipelines merge inference logic with I/O in live systems, exposing bottlenecks in performance.

### âœ… Version 3: Performance Optimization
Introduced latency-reduction strategies by:
- Downsampling video frames before inference to reduce computational load.
- Converting color space from BGR to RGB as required by TensorFlow.
- Reducing frame rate (FPS) to 15 for smoother and faster processing.

These are standard optimization practices in industry to enhance throughput and reduce inference time in live-streaming ML applications.

### âœ… Version 4: Multi-Model Pipeline Integration
Integrated image classification logic post object detection. Hand regions detected were passed into the classification model, and predicted classes were dynamically mapped to a local label dictionary. This stage represented full pipeline integration â€” a crucial step in building deployable AI systems.

### âœ… Version 5: Model Quantization for Production Efficiency
To further reduce latency introduced by classification, we:
- Converted the image classification model to a lightweight `.tflite` format.
- Applied float16 quantization to minimize model size and improve loading speed.
- Achieved lower inference latency, making the system more suitable for edge deployment (e.g., mobile or IoT).

---

## ğŸ§ª Tech Stack

- **Programming Language**: Python 3.11.13
- **Deep Learning**: TensorFlow, Keras (EfficientNet backbone)
- **Computer Vision**: OpenCV, MediaPipe
- **Optimization**: TFLite Quantization, Frame Preprocessing
- **System Tools**: Conda, pip, JSON-based label mapping

---

## ğŸ› ï¸ How to Run the Project

```bash
# Step 1: Create virtual environment
conda create -n <env_name> python=3.11.13

# Step 2: Activate environment
conda activate <env_name>

# Step 3: Install dependencies
pip install -r requirements.txt

# Step 4: Launch the pipeline
python run.py
```

---

## ğŸ“‚ Folder Structure

```
ASL_Recognition/
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ quant_models/            # TFLite-quantized models (for optimized inference)
â”‚   â””â”€â”€ normal_models/           # Standard Keras (.h5) models
â”‚
â”œâ”€â”€ version_1/
â”‚   â”œâ”€â”€ module.py                # Core logic for version 1 pipeline
â”‚   â””â”€â”€ run.py                   # Entry script for version 1
â”‚
â”œâ”€â”€ version_2/
â”‚   â”œâ”€â”€ module.py
â”‚   â””â”€â”€ run.py
â”‚
â”œâ”€â”€ version_3/
â”‚   â”œâ”€â”€ module.py
â”‚   â””â”€â”€ run.py
â”‚
â”œâ”€â”€ version_4/
â”‚   â”œâ”€â”€ module.py
â”‚   â””â”€â”€ run.py
â”‚
â”œâ”€â”€ version_5/
â”‚   â”œâ”€â”€ module.py
â”‚   â””â”€â”€ run.py
â”‚
â”œâ”€â”€ db.json                      # Class label to ASL mapping (or metadata DB)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # Project documentation
```

---

## âœ¨ Key Features

- ğŸ¥ **Real-Time Sign Detection** from video or webcam
- ğŸ§© **Version-Based Modular Pipeline** for flexibility and scalability
- ğŸ§  **Multi-Stage Model Integration** combining object detection and classification
- âš¡ **Optimized for Performance** using downsampling and frame-rate tuning
- ğŸ“¦ **Edge-Friendly Quantization** via TensorFlow Lite (float16 conversion)
- ğŸ’¡ **Production-Oriented Design** suitable for deployment and scale

---

## ğŸ§  System Architecture

```
[Video Source]
      â†“
[Frame Preprocessing]
      â†“
[Hand Detection Model]
      â†“
[Extract Hand ROI]
      â†“
[Image Classification Model]
      â†“
[Class Mapping + Display Output]
```

---

## ğŸ“ˆ Possible Extensions

- ğŸ“± Build a mobile app for ASL-to-speech
- ğŸ”Š Add text-to-speech engine for each sign
- ğŸ” Use sequence models (e.g., LSTMs or Transformers) for multi-sign detection
- ğŸŒ Host an API using Flask or FastAPI
- ğŸ’» Create a web interface with Streamlit
- ğŸ§  Train with a larger ASL dataset for more vocabulary
- ğŸš€ Deploy on edge hardware (Raspberry Pi, Jetson Nano)

---

## ğŸ‘¨â€ğŸ’» Author

**Rohith Maddikunta**  
Deep Learning & Computer Vision Enthusiast  
ğŸ“« [LinkedIn](https://www.linkedin.com/in/rohith-maddikunta/) | âœ‰ï¸ rohithmaddikunta@gmail.com

---

## ğŸ“œ License

This project is licensed under the **MIT License**.  
Use it freely with attribution.

---

## ğŸ™Œ Acknowledgements

- [TensorFlow](https://www.tensorflow.org/)
- [MediaPipe](https://mediapipe.dev/)
- [OpenCV](https://opencv.org/)
- Keras EfficientNet authors
